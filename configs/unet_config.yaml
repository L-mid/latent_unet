
# ----- Training Hyperparameters -----

training:
  batch_size: 32
  num_epochs: 300
  grad_clip: 1.0
  grad_accum: 1
  amp: true
  gradient_checkpointing: false
  num_workers: 8
  vis_interval: 10    # every N epochs
  ckpt_interval: 25   # every N epochs

checkpoint: 
  backend: "zarr"   # Options: "vanilla" | "zarr" | "tensorstore"
  save_every: 1000
  out_dir: "checkpoints/baseline_flash"  


# ---- General Settings ----

seed: 1337
device: "cuda"   # "cpu" | "cuda"
resume_path: null

model:
  name: "Unet"
  image_size: 32
  in_channels: 3
  base_channels: 128
  channel_multipliers: [1, 2, 4, 8]
  num_res_blocks: 2

attention:
  kind: "vanilla"   # "vanilla" | "linear" | "flash" | "none"
  per_block:
    down: [false, true, true, false]
    up: [false, true, false]
  params:
    num_heads: 8
    norm_groups: 8      
    dim_head: null    # Not currently used
    start_layer: 2    # apply at downsample levels 1 and 2
    window_size: 4
    midblock: true
    backend: "auto"  # auto | flash_only | fallback_only

resblock: 
  kind: "vanilla"   # also "convnext", "film", etc in future
  params: 
    norm_type: "group"
    use_scale_shift_norm: false 

time_embedding:
  kind: "sinusoidal"  # "sinusoidal" | "film" | "learned"
  params:
    dim: 512

midblock:
  use_attention: true
  num_layers: 2

updown:
  conv_type: "standard"   # could be "resample", "convnext_down", etc in future 
  use_skip_connections: true
  num_layers: 2
  expect_skip: true  # only works if true

final_head:
  use_norm: true
  activation: "SiLU"
  out_channels: 3

ema: 
  use_ema: true
  decay: 0.9999
  update_every: 1

schedule:
  schedule: "cosine"    # "linear" | "cosine"
  beta_start: 0.0001
  beta_end: 0.02
  timesteps: 1000

optim:
  optimizer: "adamw"
  lr: 3e-4
  betas: [0.9, 0.999]
  weight_decay: 0.01
  scheduler: "cosine"     # Options: "cosine" | "linear" | "constant", etc
  step_size: 1000
  gamma: 0.95


# ---- Losses ----
losses:
  - type: "mse"
    weight: 1.0
    schedule:
      type: "linear"
      start_epoch: null
      end_epoch: null    

  - type: "lpips"
    start_weight: 0.0
    end_weight: 1.0
    schedule:
      type: "linear"
      start_epoch: null
      end_epoch: null

  - type: "vgg"
    weight: 0.5
    schedule: 
      type: "cosine"
      start_step: null
      end_step: null


# ---- Sampling ----

sampler:
  type: "ddim"    # Options: "ddpm" | "ddim" | "edm"

  # Shared sampling config
  num_steps: 50
  guidance_scale: 3.5
  classifier_free: true

  # DDPM-specific (ancestral sampling)
  eta: 1.0

  # DDIM-specific
  ddim_eta: 0.0   # 0 (deterministic) | >0 (stochasticy)

  # EDM-specific
  sigma_min: 0.002
  sigma_max: 80.0
  rho: 7.0
  solver: "heun"  # "heun" | "euler", etc


# ---- Data ----

data:
  name: my_data  # (placeholder)
  root: /data/loader
  batch_size: 64
  num_workers: 8
  shuffle: true
  
  # extras (all null for now)
  perfetch: null
  tokenizer: null
  transform_type: null
  use_webdataset: null


# ---- Logging ----

logging:
  use_wandb: true
  project: "latent-unet-v1"
  run_name: "baseline_128ch_flash"


# ---- Visualization ----
  enable: true
  output_dir: ./visuals
  save_every: 500   # Save every N steps
  max_images: 16     # N of images per vis
  steps_to_plot: [0, 50, 100, 250, 500, 750, 999]

  # Vis types:
  plot_attention: true
  plot_latents: true
  plot_timesteps: true 
  # etc etc etc...


# --- Debugging ---

debug:
  enabled: true
  enable_verbose_logs: true




















