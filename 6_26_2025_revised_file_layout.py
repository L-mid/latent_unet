
"""
latent_unet_v1/
â”œâ”€â”€ run.py ðŸ”¹
â”œâ”€â”€ README.md ðŸ”¹
â”œâ”€â”€ pyproject.toml ðŸ”¹
â”œâ”€â”€ .gitignore ðŸ”¹
â”œâ”€â”€ .github/workflows/ci.yaml ðŸ”¹

â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ unet_config.yaml ðŸ”¹
â”‚   â”œâ”€â”€ test_config.yaml ðŸ”¹
â”‚   â””â”€â”€ mock_data_config.yaml ðŸ”¹

â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ config.py ðŸ”¹
â”‚   â”œâ”€â”€ build_unet.py ðŸ”¹
â”‚   â””â”€â”€ unet.py ðŸ”¹ 

â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ time_embedding.py ðŸ”¹
â”‚   â”œâ”€â”€ residual_block.py ðŸ”¹
â”‚   â”œâ”€â”€ mid_block.py ðŸ”¹
â”‚   â”œâ”€â”€ down_block.py ðŸ”¹
â”‚   â”œâ”€â”€ up_block.py ðŸ”¹
â”‚   â”œâ”€â”€ final_head.py ðŸ”¹
â”‚   â”œâ”€â”€ norm_utils.py ðŸ”¹
â”‚   â””â”€â”€ attention/ 
â”‚       â”œâ”€â”€ base_attention.py ðŸ”¹
â”‚       â”œâ”€â”€ vanilla_attention.py ðŸ”¹
â”‚       â”œâ”€â”€ window_attention.py ðŸ”¹
â”‚       â””â”€â”€ flash_attention.py ðŸ”¹

â”œâ”€â”€ diffusion/ ðŸ”¹
â”‚   â”œâ”€â”€ ddpm.py ðŸ”¹
â”‚   â”œâ”€â”€ ddim.py ðŸ”¹
â”‚   â”œâ”€â”€ edm.py ðŸ”¹
â”‚   â”œâ”€â”€ sampler_registry.py ðŸ”¹
â”‚   â”œâ”€â”€ sampler_utils.py ðŸ”¹
â”‚   â”œâ”€â”€ forward_process.py ðŸ”¹
â”‚   â””â”€â”€ schedule.py ðŸ”¹

â”œâ”€â”€ trainer/
â”‚   â”œâ”€â”€ train_loop.py XXX (working on)
â”‚   â”œâ”€â”€ cluster_utils.py ðŸ”¹
â”‚   â”œâ”€â”€ logger.py ðŸ”¹
â”‚   â”œâ”€â”€ optim_utils.py ðŸ”¹
â”‚   â”œâ”€â”€ ema_utils.py ðŸ”¹
â”‚   â””â”€â”€ losses.py ðŸ”¹

â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ loader.py ðŸ”¹
â”‚   â”œâ”€â”€ mock_dataset.py ðŸ”¹
â”‚   â”œâ”€â”€ (some others) ðŸ”¹
â”‚   â””â”€â”€ dataset_registry.py ðŸ”¹

â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ debug.py ðŸ”¹
â”‚   â”œâ”€â”€ visualizer.py ðŸ”¹
â”‚   â”œâ”€â”€ memory_tools.py
â”‚   â”œâ”€â”€ failure_injection.py
â”‚   â”œâ”€â”€ tensor_inspect.py
â”‚   â”œâ”€â”€ vanilla_checkpointing.py ðŸ”¹
â”‚   â”‚
â”‚   â”œâ”€â”€ zarr_checkpointing/ ðŸ”¹
â”‚   â”‚    â”œâ”€â”€ zarr_core.py ðŸ”¹
â”‚   â”‚    â””â”€â”€ zarr_wrapper.py ðŸ”¹
â”‚   â”‚
â”‚   â””â”€â”€ tensorstore_checkpointing/
â”‚       â”œâ”€â”€ __init__.py ðŸ”¹
â”‚       â”œâ”€â”€ tensorstore_core.py ðŸ”¹
â”‚       â”œâ”€â”€ tensorstore_wrapper.py ðŸ”¹
â”‚       â”œâ”€â”€ schema_utils.py ðŸ”¹
â”‚       â”œâ”€â”€ chunk_tuner.py ðŸ”¹
â”‚       â”œâ”€â”€ registry.py ðŸ”¹
â”‚       â””â”€â”€ remote_utils.py ðŸ”¹

â”œâ”€â”€ helpers/
â”‚   â”œâ”€â”€ mock_configs.py
â”‚   â”œâ”€â”€ fake_model.py
â”‚   â”œâ”€â”€ fake_data.py
â”‚   â””â”€â”€ test_utils.py ðŸ”¹

â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€  conftest.py ðŸ”¹
â”‚   â”‚
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_time_embedding.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ test_residual_block.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ test_attention_block.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ test_midblock.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ test_down_block.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ test_up_block.py ðŸ”¹
â”‚   â”‚   â””â”€â”€ test_final_head.py ðŸ”¹
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_unet.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ test_integrated_forward_process.py ?
â”‚   â”‚   â”œâ”€â”€ test_train_loop.py ðŸ”¹
â”‚   â”‚   â””â”€â”€ test_ddpm_ddim_edm.pyðŸ”¹
â”‚   â”‚
â”‚   â”œâ”€â”€ end_to_end/
â”‚   â”‚   â”œâ”€â”€ test_cuda_health.py
â”‚   â”‚   â”œâ”€â”€ test_training_pipeline.py
â”‚   â”‚   â””â”€â”€ test_sampling_loop.py
â”‚   â”‚
â”‚   â”œâ”€â”€ subsystems/                
â”‚   â”‚   â”œâ”€â”€ checkpointing/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_tensorstore.py ðŸ”¹
â”‚   â”‚   â”‚   â”œâ”€â”€ test_tensorstore_schema_utils.py ðŸ”¹
â”‚   â”‚   â”‚   â”œâ”€â”€ test_zarr.py ðŸ”¹
â”‚   â”‚   â”‚   â””â”€â”€ test_vanilla_checkpointing.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â””â”€â”€ test_data_pipeline.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ config/
|   |   |   â”œâ”€â”€ test_config_loading.pyðŸ”¹
â”‚   â”‚   â”‚   â”œâ”€â”€ test_config_integration.py
â”‚   â”‚   â”‚   â””â”€â”€ test_config_roundtrip.pyðŸ”¹
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ diffusion/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_schedule.py ðŸ”¹
â”‚   â”‚   â”‚   â”œâ”€â”€ test_forward_process.py ðŸ”¹
â”‚   â”‚   â”‚   â”œâ”€â”€ test_ddpm.py ðŸ”¹
â”‚   â”‚   â”‚   â”œâ”€â”€ test_ddim.py ðŸ”¹
â”‚   â”‚   â”‚   â”œâ”€â”€ test_edm.py ðŸ”¹
â”‚   â”‚   â”‚   â”œâ”€â”€ test_losses.py ðŸ”¹
â”‚   â”‚   â”‚   â””â”€â”€ test_sampler_registry.py ðŸ”¹
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ test_visualizer.py ðŸ”¹
â”‚   â”‚   â”œâ”€â”€ test_debug_hooks.py 
â”‚   â”‚   â””â”€â”€ test_failure_injection.py
â”‚   â”‚
â”‚   â”œâ”€â”€ regression/
â”‚   â”‚   â””â”€â”€ test_sequential_nan_bug.py  # (placeholder for when)
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py 





Legend:
ðŸ”¹: Implimented once - (copied direct)                      - You typed it in, but it's "dead muscle memory".
ðŸ”º: Known            - (wrote with some copying)            - Can explain dataflow, run small tests, still peaks for APIs.
ðŸ”¸: Level 1          - (can write without help)             - Can re-impliment with <3 peaks per function, tests pass.
ðŸ”¸ðŸ”¸: Level 2        - (can write and change it a bit)     - Can re-impliment model+train loop cold; contract tests all pass.       
ðŸ”¸ðŸ”¸ðŸ”¸: Level 3     - (can write and refactor)             - Can inject bugs and your asserts/localization catch them quickly; can swap varients easily without breaking.
ðŸŒŸ: Mastered         - (automatic)                          - Can design new abstractions, refactor repo-scale systems, anticipate faliure modes easily.

~82 files (not updated)

"""

# Repo mastery priorities (using Legend in above string):
"""
For tracking progress in this legend scale, these are the levels needed:

- Tensor/data contracts (shapes, sizes, flow) -> 20% ðŸ”¸ðŸ”¸ðŸ”¸ 
- Core math & algorithms (losses, diffusion schedule, EMA) -> 15% ðŸ”¸ðŸ”¸
- Architecture interfaces (ResBlock, AttentionBlock, UNet forward signatures) -> ðŸ”¸ðŸ”¸ðŸ”¸ (or ðŸŒŸ)
- Config schema & builder linkage -> 10% ðŸ”¸
- Training loop order op ops -> 10% ðŸ”¸ðŸ”¸ðŸ”¸
- Testing & invarients -> 10% ðŸ”¸ðŸ”¸ 
- External APIs (torch, wandb, tensorstore) -> 10% ðŸ”¸
- Logging/debug utilities -> 5% ðŸ”º
- CLI/plumbing -> 3% ðŸ”¹
- Setup quirks -> 2% ðŸ”¹

"""


# Add one test per subsystem or block that:
# Asserts config roundtrips
# sanity-checks output shapes
# verifies NaN/inf-free gradients
# can be run on CPU + CI only
# Parameterize CPU and GPU runs, must work on both





